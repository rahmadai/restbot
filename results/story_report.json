{
  "utter_provide_restaurant_hours": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_happy": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 3
  },
  "intent_ask_restaurant_location": {
    "precision": 0.5,
    "recall": 1.0,
    "f1-score": 0.6666666666666666,
    "support": 1
  },
  "utter_provide_menu_recommendation": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "intent_greet": {
    "precision": 0.35294117647058826,
    "recall": 1.0,
    "f1-score": 0.5217391304347826,
    "support": 6
  },
  "intent_provide_information": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "mood_great": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 2
  },
  "intent_restaurant_working_hours": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "bot_challenge": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_confirmation": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3
  },
  "intent_thanks": {
    "precision": 0.75,
    "recall": 1.0,
    "f1-score": 0.8571428571428571,
    "support": 3
  },
  "deny": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 2
  },
  "utter_provide_restaurant_location": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "intent_restaurant_menu": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_reject": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "goodbye": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 2
  },
  "utter_iamabot": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_ask_questions": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 6
  },
  "intent_recommendation_menu": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_provide_restaurant_menu": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_did_that_help": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 3
  },
  "intent_reject": {
    "precision": 0.6666666666666666,
    "recall": 1.0,
    "f1-score": 0.8,
    "support": 2
  },
  "affirm": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_confirmation_for_restaurant": {
    "precision": 1.0,
    "recall": 0.5,
    "f1-score": 0.6666666666666666,
    "support": 2
  },
  "utter_greet": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 5
  },
  "intent_recommendation_restaurant": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3
  },
  "mood_unhappy": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 3
  },
  "action_listen": {
    "precision": 1.0,
    "recall": 0.5365853658536586,
    "f1-score": 0.6984126984126985,
    "support": 41
  },
  "utter_goodbye": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 4
  },
  "greet": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 5
  },
  "utter_provide_general_recommendation_restaurant": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3
  },
  "utter_thanks": {
    "precision": 1.0,
    "recall": 0.6666666666666666,
    "f1-score": 0.8,
    "support": 3
  },
  "utter_cheer_up": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 3
  },
  "intent_confirmation": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 5
  },
  "micro avg": {
    "precision": 0.8292682926829268,
    "recall": 0.544,
    "f1-score": 0.6570048309178744,
    "support": 125
  },
  "macro avg": {
    "precision": 0.5373414071510957,
    "recall": 0.5500956480153036,
    "f1-score": 0.5297243535095196,
    "support": 125
  },
  "weighted avg": {
    "precision": 0.665607843137255,
    "recall": 0.544,
    "f1-score": 0.5706942719116632,
    "support": 125
  },
  "accuracy": 0.544,
  "conversation_accuracy": {
    "accuracy": 0.38461538461538464,
    "correct": 5,
    "with_warnings": 0,
    "total": 13
  }
}